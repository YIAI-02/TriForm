{
  "presets": {
    "llama-7b":  { "type": "llama", "layer_num": 32, "hidden_size": 4096, "intermediate_size": 11008, "num_attention_heads": 32, "num_key_value_heads": 32 },
    "llama-13b": { "type": "llama", "layer_num": 40, "hidden_size": 5120, "intermediate_size": 13824, "num_attention_heads": 40, "num_key_value_heads": 40 },
    "llama-70b": { "type": "llama", "layer_num": 80, "hidden_size": 8192, "intermediate_size": 28672, "num_attention_heads": 64, "num_key_value_heads": 8 },

    "mpt-7b":    { "type": "mpt",   "layer_num": 32, "hidden_size": 4096, "intermediate_size": 16384, "num_attention_heads": 32, "num_key_value_heads": 32 },
    "mpt-30b":   { "type": "mpt",   "layer_num": 48, "hidden_size": 7168, "intermediate_size": 28672, "num_attention_heads": 64, "num_key_value_heads": 64 },

    "palm-8b":   { "type": "palm",  "layer_num": 32,  "hidden_size": 4096,  "intermediate_size": 16384,  "num_attention_heads": 16, "num_key_value_heads": 16 },
    "palm-62b":  { "type": "palm",  "layer_num": 64,  "hidden_size": 8192,  "intermediate_size": 32768,  "num_attention_heads": 32, "num_key_value_heads": 32 },
    "palm-540b": { "type": "palm",  "layer_num": 118, "hidden_size": 18432, "intermediate_size": 73728,  "num_attention_heads": 48, "num_key_value_heads": 48 }
  },
  "aliases": {
    "llama2-7b":  "llama-7b",
    "llama2-13b": "llama-13b",
    "llama2-70b": "llama-70b"
  }
}
