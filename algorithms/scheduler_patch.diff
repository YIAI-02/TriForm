diff --git a/algorithms/scheduler.py b/algorithms/scheduler.py
index d09c9548d2bf71bc913646e92f314259d552ad29..737ac8106fd8543b181acdd274ea5006d31bbf23 100644
--- a/algorithms/scheduler.py
+++ b/algorithms/scheduler.py
@@ -125,79 +125,113 @@ class HEFTScheduler:
         devs = list(self.cluster.devices.values())
         total_compute = 0.0
         total_w = 0.0
         k = 0
         seq_len = int(getattr(self, "seq_len", 0) or 0)
         batch = int(getattr(self, "batch", 0) or 0)
         node_flops = self.cost.estimate_flops(node, batch, seq_len, phase)
         node_weight_size = node.weight_size
         for d in devs:
             if not node.allowed.get(d.type, True):
                 continue
             k += 1
             device_compute = self.cost.flop_time(node_flops, d)
             total_compute += device_compute
             if RANKU_INCLUDE_AVG_WEIGHT_LOAD and node.weight_id and node_weight_size > 0:
                 wid = node.weight_id
                 stored_fmt = self.storage_fmt_map.get(wid, "ND")
                 size_src = self.cost.format_size(int(node_weight_size), stored_fmt)
                 weight_cost = self.cost.gb_move_and_format(d, size_src, stored_fmt, self.cost.device_preferred_fmt(d))
                 total_w += weight_cost
         avg_compute = (total_compute / k) if k else 0.0
         avg_w = (total_w / k) if (k and RANKU_INCLUDE_AVG_WEIGHT_LOAD and node.weight_id) else 0.0
         total_avg = avg_compute + avg_w
         return total_avg
 
-    def _avg_comm_cost(self, u: TaskNode, v: TaskNode) -> float:
-        devs = list(self.cluster.devices.values())
-        total = 0.0
-        k = 0
-        bytes_nd = max(u.bytes_write, v.bytes_read, 16 * 1024)
-        seq_len = getattr(self, "seq_len", None)
-        batch = getattr(self, "batch", None)
-        if hasattr(u, "attrs") and u.attrs:
-            base_seq = u.attrs.get("seq_len", None)
-            base_batch = u.attrs.get("batch", None)
-            if base_seq and seq_len and base_seq > 0 and seq_len != base_seq:
-                bytes_nd = bytes_nd * seq_len / base_seq
-            if base_batch and batch and base_batch > 0 and batch != base_batch:
-                bytes_nd = bytes_nd * batch / base_batch
-        for i in range(len(devs)):
-            for j in range(len(devs)):
-                di, dj = devs[i], devs[j]
-                if not (u.allowed.get(di.type, True) and v.allowed.get(dj.type, True)):
-                    continue
-                src_fmt = self.cost.device_preferred_fmt(di)
-                dst_fmt = self.cost.device_preferred_fmt(dj)
-                payload_src = self.cost.format_size(int(bytes_nd), src_fmt)
-                t_link = self.cost.comm_cost(di, dj, payload_src)
-                t_conv = 0.0
-                if di.type != dj.type:
-                    t_conv = self.cost.format_conversion_time(payload_src, src_fmt, dst_fmt, dj)
-                total += (t_link + t_conv)
-                k += 1
-        return total / k if k else 0.0
+    def _avg_comm_cost(self, u: TaskNode, v: TaskNode) -> float:
+        devs = list(self.cluster.devices.values())
+        total = 0.0
+        k = 0
+        u_read, u_write = self._estimate_node_io_bytes(u)
+        v_read, _ = self._estimate_node_io_bytes(v)
+        payload_bytes = max(u_write, v_read, 16 * 1024)
+        for i in range(len(devs)):
+            for j in range(len(devs)):
+                di, dj = devs[i], devs[j]
+                if not (u.allowed.get(di.type, True) and v.allowed.get(dj.type, True)):
+                    continue
+                src_fmt = self.cost.device_preferred_fmt(di)
+                dst_fmt = self.cost.device_preferred_fmt(dj)
+                payload_src = self.cost.format_size(int(payload_bytes), src_fmt)
+                t_link = self.cost.comm_cost(di, dj, payload_src)
+                t_conv = 0.0
+                if di.type != dj.type:
+                    t_conv = self.cost.format_conversion_time(payload_src, src_fmt, dst_fmt, dj)
+                total += (t_link + t_conv)
+                k += 1
+        return total / k if k else 0.0
+
+    def _estimate_node_io_bytes(
+        self,
+        node: TaskNode,
+        *,
+        batch: Optional[int] = None,
+        seq_len: Optional[int] = None,
+    ) -> Tuple[int, int]:
+        batch = int(batch if batch is not None else getattr(self, "batch", 0) or 0)
+        seq_len = int(seq_len if seq_len is not None else getattr(self, "seq_len", 0) or 0)
+        attrs = getattr(node, "attrs", {}) or {}
+        base_seq = int(attrs.get("seq_len", attrs.get("base_seq_len", 0)) or 0)
+        base_batch = int(attrs.get("batch", attrs.get("base_batch", 0)) or 0)
+
+        if seq_len <= 0:
+            seq_len = base_seq if base_seq > 0 else 1
+        if batch <= 0:
+            batch = base_batch if base_batch > 0 else 1
+
+        read = int(getattr(node, "bytes_read", 0) or 0)
+        write = int(getattr(node, "bytes_write", 0) or 0)
+
+        # Dynamic estimation for KV ops uses dedicated helper
+        if (node.name or "").lower() in {"kv_read", "kv_write"}:
+            r, w = self.cost.kv_rw_bytes_decode(node, max(1, batch), max(1, seq_len))
+            if (node.name or "").lower() == "kv_read":
+                return max(r, 0), max(r, 0)
+            return max(w, 0), max(w, 0)
+
+        def scale(value: int) -> int:
+            out = float(value)
+            if base_seq > 0 and seq_len > 0:
+                out *= seq_len / float(base_seq)
+            if base_batch > 0 and batch > 0:
+                out *= batch / float(base_batch)
+            return int(max(out, 0.0))
+
+        read = scale(read)
+        write = scale(write)
+
+        return read, write
 
     def _upward_rank(self, g: TaskGraph, phase: str) -> List[str]:
         # compute rank_u bottom-up
         succ = {nid: list(g.successors(nid)) for nid in g.nodes}
         order = list(reversed(g.topological()))
         rank_u: Dict[str, float] = {}
         for nid in order:
             node = g.nodes[nid]
             if not succ[nid]:
                 # Leaf node - only compute cost
                 compute_cost = self._avg_compute_cost(node, phase=phase)
                 rank_u[nid] = compute_cost
             else:
                 # Non-leaf node - compute + max path to successors
                 compute_cost = self._avg_compute_cost(node, phase=phase)
                 best = 0.0
                
                 for v in succ[nid]:
                     comm_cost = self._avg_comm_cost(node, g.nodes[v])
                     path_cost = comm_cost + rank_u[v]
                     if path_cost > best:
                         best = path_cost
                 
                 rank_u[nid] = compute_cost + best
         # schedule order: descending rank_u
@@ -266,116 +300,158 @@ class HEFTScheduler:
 
         return max(0.0, end - t0)
 
     def _kv_transfer_time_if_needed(self, node: TaskNode, dev: DeviceSpec, phase: str, t0: float, commit: bool) -> float:
         """Only for decode on PIM in small mode: host<->PIM KV movement."""
         if phase != "decode":
             return 0.0
         if node.name not in ("KV_read", "KV_write"):
             return 0.0
         if dev.type != "pim":
             return 0.0
         if getattr(self.label, "kv_in_pim", False):
             return 0.0
 
         r, w = self.cost.kv_rw_bytes_decode(node, self.batch, self.seq_len)
         host = self.cost.get_host_device().name
         if node.name == "KV_read":
             _, end = self.comm.reserve(host, dev.name, r, earliest=t0, commit=commit)
         else:
             _, end = self.comm.reserve(dev.name, host, w, earliest=t0, commit=commit)
         return max(0.0, end - t0)
 
     def _earliest_finish_on_device(self, g: TaskGraph, nid: str, dev: DeviceSpec, phase: str, commit: bool) -> Tuple[float, float]:
         node = g.nodes[nid]
 
-        # 1) inputs ready (consider cross-device comm + dst format conversion)
-        ready_time = 0.0
-        for u in g.predecessors(nid):
-            pred_finish = self._node_finish_time.get(u, 0.0)
-            pred_dev_name = self._node_placement.get(u, dev.name)
-            pred_dev = self.cluster.devices[pred_dev_name]
-            if pred_dev.name == dev.name:
-                ready_time = max(ready_time, pred_finish)
-            else:
-                payload_nd = max(g.nodes[u].bytes_write, node.bytes_read, 16 * 1024)
-                src_fmt = self._node_out_fmt.get(u, self.cost.device_preferred_fmt(pred_dev))
-                dst_fmt = self.cost.device_preferred_fmt(dev)
-                payload_src = self.cost.format_size(payload_nd, src_fmt)
-                _, link_end = self.comm.reserve(pred_dev.name, dev.name, payload_src, earliest=pred_finish, commit=commit)
-                dep_end = link_end + self.cost.format_conversion_time(payload_src, src_fmt, dst_fmt, dev)
-                ready_time = max(ready_time, dep_end)
-
-        # 2) device available
-        t0 = max(self.avail[dev.name], ready_time)
-
-        # 3) compute
-        compute_t = self.cost.node_device_cost(node, dev, self.batch, self.seq_len, phase)
-
-        # 4) overlappable transfers
-        wload_t = self._weight_load_time(node, dev, t0, commit)
-        kv_t = self._kv_transfer_time_if_needed(node, dev, phase, t0, commit)
-
-        # 5) finish
-        finish = t0 + max(compute_t, wload_t, kv_t)
-        if commit:
-            self._node_out_fmt[nid] = self.cost.device_preferred_fmt(dev)
-        return t0, finish
+        # 1) inputs ready (consider cross-device comm + dst format conversion)
+        inbound_start_times: List[float] = []
+        inbound_end_times: List[float] = []
+        node_read, _ = self._estimate_node_io_bytes(node)
+        for u in g.predecessors(nid):
+            pred_finish = self._node_finish_time.get(u, 0.0)
+            pred_dev_name = self._node_placement.get(u, dev.name)
+            pred_dev = self.cluster.devices[pred_dev_name]
+            if pred_dev.name == dev.name:
+                inbound_start_times.append(pred_finish)
+                inbound_end_times.append(pred_finish)
+                continue
+
+            src_fmt = self._node_out_fmt.get(u, self.cost.device_preferred_fmt(pred_dev))
+            dst_fmt = self.cost.device_preferred_fmt(dev)
+            pred_node = g.nodes[u]
+            _, pred_write = self._estimate_node_io_bytes(pred_node)
+            payload_nd = max(pred_write, node_read, 16 * 1024)
+            payload_src = self.cost.format_size(payload_nd, src_fmt)
+            link_start, link_end = self.comm.reserve(
+                pred_dev.name,
+                dev.name,
+                payload_src,
+                earliest=pred_finish,
+                commit=commit,
+            )
+            conv_t = self.cost.format_conversion_time(payload_src, src_fmt, dst_fmt, dev)
+            dep_end = link_end + conv_t
+            inbound_start_times.append(link_start)
+            inbound_end_times.append(dep_end)
+
+        if dev.type == "npu":
+            ready_time = max(inbound_start_times, default=0.0)
+        else:
+            ready_time = max(inbound_end_times, default=0.0)
+
+        # 2) device available
+        t0 = max(self.avail[dev.name], ready_time)
+
+        # 3) compute
+        compute_t = self.cost.node_device_cost(node, dev, self.batch, self.seq_len, phase)
+
+        # 4) overlappable transfers
+        wload_t = self._weight_load_time(node, dev, t0, commit)
+        kv_t = self._kv_transfer_time_if_needed(node, dev, phase, t0, commit)
+
+        # 5) finish with device-specific overlap model
+        if dev.type == "npu":
+            inbound_overlap = 0.0
+            if inbound_end_times:
+                max_inbound_end = max(inbound_end_times)
+                inbound_overlap = max(0.0, max_inbound_end - t0)
+            total = max(compute_t, wload_t, kv_t, inbound_overlap)
+            finish = t0 + total
+        else:
+            cursor = t0
+            cursor += wload_t
+            cursor += compute_t
+            cursor += kv_t
+            finish = cursor
+        if commit:
+            self._node_out_fmt[nid] = self.cost.device_preferred_fmt(dev)
+        return t0, finish
 
     # --------------------------
     # Hybrid helpers
     # --------------------------
     def _earliest_free_device(self, dev_type: str) -> Tuple[Optional[DeviceSpec], float]:
         devs = self.cluster.devices_by_type(dev_type)
         if not devs:
             return None, float("inf")
         best = None
         best_t = float("inf")
         for d in devs:
             t = self.avail.get(d.name, 0.0)
             if t < best_t:
                 best, best_t = d, t
         return best, best_t
 
-    def _ready_time_for_device(self, g: TaskGraph, nid: str, dev: DeviceSpec, phase: str, commit: bool) -> float:
-        node = g.nodes[nid]
-        ready = 0.0
-        for u in g.predecessors(nid):
-            pred_finish = self._node_finish_time.get(u, 0.0) #前驱节点完成时间
-            pred_dev_name = self._node_placement.get(u, dev.name) #前驱节点被分配的设备名称
-            pred_dev = self.cluster.devices[pred_dev_name] #前驱节点被分配的设备
-            if pred_dev.name == dev.name: #同设备
-                ready = max(ready, pred_finish)
-            else:
-                payload_nd = max(g.nodes[u].bytes_write, node.bytes_read, 16 * 1024)
-                src_fmt = self._node_out_fmt.get(u, self.cost.device_preferred_fmt(pred_dev))
-                dst_fmt = self.cost.device_preferred_fmt(dev)
-                payload_src = self.cost.format_size(payload_nd, src_fmt)
-                _, link_end = self.comm.reserve(pred_dev.name, dev.name, payload_src, earliest=pred_finish, commit=commit)
-                dep_end = link_end + self.cost.format_conversion_time(payload_src, src_fmt, dst_fmt, dev)
-                ready = max(ready, dep_end)
-        return ready
+    def _ready_time_for_device(self, g: TaskGraph, nid: str, dev: DeviceSpec, phase: str, commit: bool) -> float:
+        node = g.nodes[nid]
+        inbound_start_times: List[float] = []
+        inbound_end_times: List[float] = []
+        node_read, _ = self._estimate_node_io_bytes(node)
+        for u in g.predecessors(nid):
+            pred_finish = self._node_finish_time.get(u, 0.0) #前驱节点完成时间
+            pred_dev_name = self._node_placement.get(u, dev.name) #前驱节点被分配的设备名称
+            pred_dev = self.cluster.devices[pred_dev_name] #前驱节点被分配的设备
+            if pred_dev.name == dev.name: #同设备
+                inbound_start_times.append(pred_finish)
+                inbound_end_times.append(pred_finish)
+                continue
+
+            src_fmt = self._node_out_fmt.get(u, self.cost.device_preferred_fmt(pred_dev))
+            dst_fmt = self.cost.device_preferred_fmt(dev)
+            pred_node = g.nodes[u]
+            _, pred_write = self._estimate_node_io_bytes(pred_node)
+            payload_nd = max(pred_write, node_read, 16 * 1024)
+            payload_src = self.cost.format_size(payload_nd, src_fmt)
+            link_start, link_end = self.comm.reserve(pred_dev.name, dev.name, payload_src, earliest=pred_finish, commit=commit)
+            conv_t = self.cost.format_conversion_time(payload_src, src_fmt, dst_fmt, dev)
+            dep_end = link_end + conv_t
+            inbound_start_times.append(link_start)
+            inbound_end_times.append(dep_end)
+
+        if dev.type == "npu":
+            return max(inbound_start_times, default=0.0)
+        return max(inbound_end_times, default=0.0)
 
     def _earliest_finish_hybrid(self, g: TaskGraph, nid: str, phase: str, commit: bool) -> Optional[Dict[str, object]]:
         node = g.nodes[nid]
         npu_dev, t_npu_free = self._earliest_free_device("npu")
         pim_dev, t_pim_free = self._earliest_free_device("pim")
         if (npu_dev is None) or (pim_dev is None):
             return None
 
         t_ready_npu = self._ready_time_for_device(g, nid, npu_dev, phase, commit)
         t_ready_pim = self._ready_time_for_device(g, nid, pim_dev, phase, commit)
 
         t_w_npu = self._weight_load_time(node, npu_dev, t_npu_free, commit)
         t_w_pim = self._weight_load_time(node, pim_dev, t_pim_free, commit)
         t_kv_npu = self._kv_transfer_time_if_needed(node, npu_dev, phase, t_npu_free, commit)
         t_kv_pim = self._kv_transfer_time_if_needed(node, pim_dev, phase, t_pim_free, commit)
 
         start_npu = max(t_npu_free, t_ready_npu, t_npu_free + t_w_npu, t_npu_free + t_kv_npu)
         start_pim = max(t_pim_free, t_ready_pim, t_pim_free + t_w_pim, t_pim_free + t_kv_pim)
 
         tN = self.cost.node_device_cost(node, npu_dev, self.batch, self.seq_len, phase)
         tP = self.cost.node_device_cost(node, pim_dev, self.batch, self.seq_len, phase)
         rN = (0.0 if tN <= 0.0 else 1.0 / tN)
         rP = (0.0 if tP <= 0.0 else 1.0 / tP)
 
         if start_npu <= start_pim: